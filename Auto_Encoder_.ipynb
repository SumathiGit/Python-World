{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto_Encoder .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPr19rFPRQqnPtdiBtTFgO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumathiGit/Python-World/blob/main/Auto_Encoder_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-dSMrvvDA79"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.parallel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeEinlOHtRzQ"
      },
      "source": [
        "# dataset source https://grouplens.org/datasets/movielens/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AeWxjRRDvTj"
      },
      "source": [
        "movies = pd.read_csv(\"data/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv(\"data/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv(\"data/ratings.dat\", sep = '::::::', header = None, engine = 'python')"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MCDLgdEc7n"
      },
      "source": [
        "#Preparing training and testing Datasets\n",
        "\n",
        "training_set = pd.read_csv(\"data/u1.base\", delimiter='\\t')\n",
        "training_set = np.array(training_set, dtype='int')\n",
        "\n",
        "testing_set = pd.read_csv(\"data/u1.test\", delimiter='\\t')\n",
        "testing_set = np.array(testing_set, dtype='int')\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7-1pnLeEp12"
      },
      "source": [
        "#Getting the number of users and movies\n",
        "\n",
        "\n",
        "no_of_users = int(max(max(training_set[:,0]),max(testing_set[:,0])))\n",
        "no_of_movies = int(max(max(training_set[:,1]),max(testing_set[:,1])))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItthR7EmFC9D"
      },
      "source": [
        "#Converting the data ito matrix  with user and movies column\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for user in range(1, no_of_users+1):\n",
        "        movieid = data[:,1][data[:,0] == user]\n",
        "        ratingid = data[:,2][data[:,0] == user]\n",
        "        ratings = np.zeros(no_of_movies)\n",
        "        ratings[movieid -1] = ratingid\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hDFmtXFI27"
      },
      "source": [
        "training_set = convert(training_set)\n",
        "testing_set = convert(testing_set)\n",
        "\n",
        "#So we get out training_set and testing_set in a list (2dim)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsxv3iC2aJg8",
        "outputId": "da286eb3-90dc-4e9f-b0df-90c80a93fb6e"
      },
      "source": [
        "training_set = torch.FloatTensor(training_set)\n",
        "testing_set = torch.FloatTensor(testing_set)\n",
        "\n",
        "print(testing_set.type)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in method type of Tensor object at 0x7f4a3fe6c410>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "HdGHzTpEFNH4",
        "outputId": "0b8a3736-9313-4de5-b3de-32ba3d80e6fa"
      },
      "source": [
        "\"\"\"\n",
        "AutoEncoder \n",
        "x >> input\n",
        "f(w*x+b) = predicted values #Multiply the input with weight adding bias with a activation function we will get the pred values\n",
        "Comparing the predicted values with a actual values until we get a same output as same as the input\n",
        "finding the loss \n",
        "\"\"\""
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAutoEncoder \\nx >> input\\nf(w*x+b) = predicted values #Multiply the input with weight adding bias with a activation function we will get the pred values\\nComparing the predicted values with a actual values until we get a same output as same as the input\\nfinding the loss \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LH4fXCwFNc6"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(no_of_movies, 20)  #passing the users ratings (user1 , rating for all movies ..) as a input with 20 neurons in hidden layer that produce 20 output     \n",
        "        self.fc2 = nn.Linear(20, 10)            #Pass the 20 outputs with 10 hidden neuron \n",
        "        self.fc3 = nn.Linear(10, 20)            #Decoding \n",
        "        self.fc4 = nn.Linear(20, no_of_movies)  #Goal is to get the input match eith the output ((no_of_movies))\n",
        "        \n",
        "        self.activation = nn.Sigmoid()     #we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1 \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        return (x)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_j5Q0tlU_Dh"
      },
      "source": [
        "autoencoder = AutoEncoder()\n",
        "criterion = nn.MSELoss()    # Mean squared error is calculated as the average of the squared differences between the predicted and actual values. \n",
        "optimizer = torch.optim.RMSprop(autoencoder.parameters(), lr = 0.01, weight_decay= 0.5) #root mean square propagation "
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKLxbDV6V0Ov",
        "outputId": "a2f3e0d0-ac46-4670-f840-bd6811772fc1"
      },
      "source": [
        "#Train the autoencoder\n",
        "epochs = 200\n",
        "for epoch in range(1, epochs +1):      \n",
        "    for user in range(no_of_users):\n",
        "        input = Variable(training_set[user]).unsqueeze(0)  #unsqeeze will add an extra column in the training_set ==> single dim)>> Variable need multi dim data\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0 ) > 0 :\n",
        "          output =autoencoder(input)\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()                                  #Step to adjust ypur weight in each iteration\n",
        "    print(\"Epoch: \" +str(epoch)+\" loss:\" +str(loss.data))\n",
        "    "
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 loss:tensor(1.0077)\n",
            "Epoch: 2 loss:tensor(0.9062)\n",
            "Epoch: 3 loss:tensor(0.8904)\n",
            "Epoch: 4 loss:tensor(0.8854)\n",
            "Epoch: 5 loss:tensor(0.9080)\n",
            "Epoch: 6 loss:tensor(0.8847)\n",
            "Epoch: 7 loss:tensor(0.8878)\n",
            "Epoch: 8 loss:tensor(0.8895)\n",
            "Epoch: 9 loss:tensor(0.8836)\n",
            "Epoch: 10 loss:tensor(0.8808)\n",
            "Epoch: 11 loss:tensor(0.9064)\n",
            "Epoch: 12 loss:tensor(0.8844)\n",
            "Epoch: 13 loss:tensor(0.8987)\n",
            "Epoch: 14 loss:tensor(0.8812)\n",
            "Epoch: 15 loss:tensor(0.8808)\n",
            "Epoch: 16 loss:tensor(0.8985)\n",
            "Epoch: 17 loss:tensor(0.8950)\n",
            "Epoch: 18 loss:tensor(0.8795)\n",
            "Epoch: 19 loss:tensor(0.8842)\n",
            "Epoch: 20 loss:tensor(0.8814)\n",
            "Epoch: 21 loss:tensor(0.9200)\n",
            "Epoch: 22 loss:tensor(0.8981)\n",
            "Epoch: 23 loss:tensor(0.9287)\n",
            "Epoch: 24 loss:tensor(0.8813)\n",
            "Epoch: 25 loss:tensor(0.8848)\n",
            "Epoch: 26 loss:tensor(0.9169)\n",
            "Epoch: 27 loss:tensor(0.8780)\n",
            "Epoch: 28 loss:tensor(0.8822)\n",
            "Epoch: 29 loss:tensor(0.9272)\n",
            "Epoch: 30 loss:tensor(0.8978)\n",
            "Epoch: 31 loss:tensor(0.8843)\n",
            "Epoch: 32 loss:tensor(0.8811)\n",
            "Epoch: 33 loss:tensor(0.8785)\n",
            "Epoch: 34 loss:tensor(0.9048)\n",
            "Epoch: 35 loss:tensor(0.8902)\n",
            "Epoch: 36 loss:tensor(0.9133)\n",
            "Epoch: 37 loss:tensor(0.8786)\n",
            "Epoch: 38 loss:tensor(0.8789)\n",
            "Epoch: 39 loss:tensor(0.8967)\n",
            "Epoch: 40 loss:tensor(0.9099)\n",
            "Epoch: 41 loss:tensor(0.8927)\n",
            "Epoch: 42 loss:tensor(0.8709)\n",
            "Epoch: 43 loss:tensor(0.8788)\n",
            "Epoch: 44 loss:tensor(0.8686)\n",
            "Epoch: 45 loss:tensor(0.8688)\n",
            "Epoch: 46 loss:tensor(0.8760)\n",
            "Epoch: 47 loss:tensor(0.8792)\n",
            "Epoch: 48 loss:tensor(0.8657)\n",
            "Epoch: 49 loss:tensor(0.8597)\n",
            "Epoch: 50 loss:tensor(0.8535)\n",
            "Epoch: 51 loss:tensor(0.8458)\n",
            "Epoch: 52 loss:tensor(0.8440)\n",
            "Epoch: 53 loss:tensor(0.8391)\n",
            "Epoch: 54 loss:tensor(0.8314)\n",
            "Epoch: 55 loss:tensor(0.8288)\n",
            "Epoch: 56 loss:tensor(0.8273)\n",
            "Epoch: 57 loss:tensor(0.8329)\n",
            "Epoch: 58 loss:tensor(0.8289)\n",
            "Epoch: 59 loss:tensor(0.8235)\n",
            "Epoch: 60 loss:tensor(0.8163)\n",
            "Epoch: 61 loss:tensor(0.8220)\n",
            "Epoch: 62 loss:tensor(0.8358)\n",
            "Epoch: 63 loss:tensor(0.8385)\n",
            "Epoch: 64 loss:tensor(0.8570)\n",
            "Epoch: 65 loss:tensor(0.8382)\n",
            "Epoch: 66 loss:tensor(0.8221)\n",
            "Epoch: 67 loss:tensor(0.8451)\n",
            "Epoch: 68 loss:tensor(0.8228)\n",
            "Epoch: 69 loss:tensor(0.8204)\n",
            "Epoch: 70 loss:tensor(0.8759)\n",
            "Epoch: 71 loss:tensor(0.8483)\n",
            "Epoch: 72 loss:tensor(0.8245)\n",
            "Epoch: 73 loss:tensor(0.8142)\n",
            "Epoch: 74 loss:tensor(0.8105)\n",
            "Epoch: 75 loss:tensor(0.8141)\n",
            "Epoch: 76 loss:tensor(0.8080)\n",
            "Epoch: 77 loss:tensor(0.8162)\n",
            "Epoch: 78 loss:tensor(0.8156)\n",
            "Epoch: 79 loss:tensor(0.8122)\n",
            "Epoch: 80 loss:tensor(0.8178)\n",
            "Epoch: 81 loss:tensor(0.8178)\n",
            "Epoch: 82 loss:tensor(0.8182)\n",
            "Epoch: 83 loss:tensor(0.8195)\n",
            "Epoch: 84 loss:tensor(0.8180)\n",
            "Epoch: 85 loss:tensor(0.8206)\n",
            "Epoch: 86 loss:tensor(0.8196)\n",
            "Epoch: 87 loss:tensor(0.8247)\n",
            "Epoch: 88 loss:tensor(0.8227)\n",
            "Epoch: 89 loss:tensor(0.8363)\n",
            "Epoch: 90 loss:tensor(0.8274)\n",
            "Epoch: 91 loss:tensor(0.8540)\n",
            "Epoch: 92 loss:tensor(0.8685)\n",
            "Epoch: 93 loss:tensor(0.8635)\n",
            "Epoch: 94 loss:tensor(0.8262)\n",
            "Epoch: 95 loss:tensor(0.8336)\n",
            "Epoch: 96 loss:tensor(0.8297)\n",
            "Epoch: 97 loss:tensor(0.8574)\n",
            "Epoch: 98 loss:tensor(0.8543)\n",
            "Epoch: 99 loss:tensor(0.8346)\n",
            "Epoch: 100 loss:tensor(0.8246)\n",
            "Epoch: 101 loss:tensor(0.8187)\n",
            "Epoch: 102 loss:tensor(0.8453)\n",
            "Epoch: 103 loss:tensor(0.8377)\n",
            "Epoch: 104 loss:tensor(0.8417)\n",
            "Epoch: 105 loss:tensor(0.8467)\n",
            "Epoch: 106 loss:tensor(0.8119)\n",
            "Epoch: 107 loss:tensor(0.8208)\n",
            "Epoch: 108 loss:tensor(0.8072)\n",
            "Epoch: 109 loss:tensor(0.8048)\n",
            "Epoch: 110 loss:tensor(0.8111)\n",
            "Epoch: 111 loss:tensor(0.8187)\n",
            "Epoch: 112 loss:tensor(0.8239)\n",
            "Epoch: 113 loss:tensor(0.8385)\n",
            "Epoch: 114 loss:tensor(0.8086)\n",
            "Epoch: 115 loss:tensor(0.8020)\n",
            "Epoch: 116 loss:tensor(0.8071)\n",
            "Epoch: 117 loss:tensor(0.8048)\n",
            "Epoch: 118 loss:tensor(0.8019)\n",
            "Epoch: 119 loss:tensor(0.8034)\n",
            "Epoch: 120 loss:tensor(0.8044)\n",
            "Epoch: 121 loss:tensor(0.8042)\n",
            "Epoch: 122 loss:tensor(0.8087)\n",
            "Epoch: 123 loss:tensor(0.8110)\n",
            "Epoch: 124 loss:tensor(0.8034)\n",
            "Epoch: 125 loss:tensor(0.8189)\n",
            "Epoch: 126 loss:tensor(0.8102)\n",
            "Epoch: 127 loss:tensor(0.8073)\n",
            "Epoch: 128 loss:tensor(0.8046)\n",
            "Epoch: 129 loss:tensor(0.8118)\n",
            "Epoch: 130 loss:tensor(0.8124)\n",
            "Epoch: 131 loss:tensor(0.8215)\n",
            "Epoch: 132 loss:tensor(0.8202)\n",
            "Epoch: 133 loss:tensor(0.8135)\n",
            "Epoch: 134 loss:tensor(0.8100)\n",
            "Epoch: 135 loss:tensor(0.8113)\n",
            "Epoch: 136 loss:tensor(0.8162)\n",
            "Epoch: 137 loss:tensor(0.8147)\n",
            "Epoch: 138 loss:tensor(0.8193)\n",
            "Epoch: 139 loss:tensor(0.8140)\n",
            "Epoch: 140 loss:tensor(0.8202)\n",
            "Epoch: 141 loss:tensor(0.8109)\n",
            "Epoch: 142 loss:tensor(0.8163)\n",
            "Epoch: 143 loss:tensor(0.8111)\n",
            "Epoch: 144 loss:tensor(0.8103)\n",
            "Epoch: 145 loss:tensor(0.8112)\n",
            "Epoch: 146 loss:tensor(0.8116)\n",
            "Epoch: 147 loss:tensor(0.8100)\n",
            "Epoch: 148 loss:tensor(0.8115)\n",
            "Epoch: 149 loss:tensor(0.8152)\n",
            "Epoch: 150 loss:tensor(0.8131)\n",
            "Epoch: 151 loss:tensor(0.8148)\n",
            "Epoch: 152 loss:tensor(0.8155)\n",
            "Epoch: 153 loss:tensor(0.8146)\n",
            "Epoch: 154 loss:tensor(0.8115)\n",
            "Epoch: 155 loss:tensor(0.8146)\n",
            "Epoch: 156 loss:tensor(0.8147)\n",
            "Epoch: 157 loss:tensor(0.8136)\n",
            "Epoch: 158 loss:tensor(0.8168)\n",
            "Epoch: 159 loss:tensor(0.8366)\n",
            "Epoch: 160 loss:tensor(0.8132)\n",
            "Epoch: 161 loss:tensor(0.8134)\n",
            "Epoch: 162 loss:tensor(0.8080)\n",
            "Epoch: 163 loss:tensor(0.8088)\n",
            "Epoch: 164 loss:tensor(0.8101)\n",
            "Epoch: 165 loss:tensor(0.8103)\n",
            "Epoch: 166 loss:tensor(0.8136)\n",
            "Epoch: 167 loss:tensor(0.8510)\n",
            "Epoch: 168 loss:tensor(0.8705)\n",
            "Epoch: 169 loss:tensor(0.8020)\n",
            "Epoch: 170 loss:tensor(0.8233)\n",
            "Epoch: 171 loss:tensor(0.8230)\n",
            "Epoch: 172 loss:tensor(0.8325)\n",
            "Epoch: 173 loss:tensor(0.8317)\n",
            "Epoch: 174 loss:tensor(0.8130)\n",
            "Epoch: 175 loss:tensor(0.8127)\n",
            "Epoch: 176 loss:tensor(0.8181)\n",
            "Epoch: 177 loss:tensor(0.8170)\n",
            "Epoch: 178 loss:tensor(0.8022)\n",
            "Epoch: 179 loss:tensor(0.7951)\n",
            "Epoch: 180 loss:tensor(0.7951)\n",
            "Epoch: 181 loss:tensor(0.8097)\n",
            "Epoch: 182 loss:tensor(0.8057)\n",
            "Epoch: 183 loss:tensor(0.8062)\n",
            "Epoch: 184 loss:tensor(0.8286)\n",
            "Epoch: 185 loss:tensor(0.8262)\n",
            "Epoch: 186 loss:tensor(0.8304)\n",
            "Epoch: 187 loss:tensor(0.8104)\n",
            "Epoch: 188 loss:tensor(0.8110)\n",
            "Epoch: 189 loss:tensor(0.8110)\n",
            "Epoch: 190 loss:tensor(0.8096)\n",
            "Epoch: 191 loss:tensor(0.8098)\n",
            "Epoch: 192 loss:tensor(0.8107)\n",
            "Epoch: 193 loss:tensor(0.8093)\n",
            "Epoch: 194 loss:tensor(0.8103)\n",
            "Epoch: 195 loss:tensor(0.8005)\n",
            "Epoch: 196 loss:tensor(0.8106)\n",
            "Epoch: 197 loss:tensor(0.7991)\n",
            "Epoch: 198 loss:tensor(0.7951)\n",
            "Epoch: 199 loss:tensor(0.7950)\n",
            "Epoch: 200 loss:tensor(0.7945)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_XyJr3-a-z8"
      },
      "source": [
        "#Initially the loss is 1 after that it gradually decreased to 0.79"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMlDcEwyrd1w"
      },
      "source": [
        "#Testing the trained autoencoder using testing_set to check whether the loss is minimized or not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZDaEU0rro0u",
        "outputId": "3064569c-6c99-430f-bdc7-d5cc065d5538"
      },
      "source": [
        "total = 0\n",
        "for user in range(no_of_users):\n",
        "        input = Variable(training_set[user]).unsqueeze(0)  \n",
        "        target = Variable(testing_set[user])      #Passing the testing_set\n",
        "        if torch.sum(target.data > 0 ) > 0 :\n",
        "          output =autoencoder(input)\n",
        "          loss = criterion(output, target)                                 \n",
        "          print(\" loss:\" +str(loss.data))\n",
        "    "
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1682])) that is different to the input size (torch.Size([1, 1682])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " loss:tensor(0.9525)\n",
            " loss:tensor(0.1882)\n",
            " loss:tensor(0.1100)\n",
            " loss:tensor(0.1121)\n",
            " loss:tensor(0.4907)\n",
            " loss:tensor(0.6513)\n",
            " loss:tensor(1.4954)\n",
            " loss:tensor(0.2651)\n",
            " loss:tensor(0.1293)\n",
            " loss:tensor(0.7980)\n",
            " loss:tensor(0.6004)\n",
            " loss:tensor(0.2919)\n",
            " loss:tensor(1.8375)\n",
            " loss:tensor(0.4742)\n",
            " loss:tensor(0.2665)\n",
            " loss:tensor(0.7259)\n",
            " loss:tensor(0.0674)\n",
            " loss:tensor(0.8812)\n",
            " loss:tensor(0.0862)\n",
            " loss:tensor(0.1826)\n",
            " loss:tensor(0.4066)\n",
            " loss:tensor(0.5024)\n",
            " loss:tensor(0.5235)\n",
            " loss:tensor(0.3659)\n",
            " loss:tensor(0.4384)\n",
            " loss:tensor(0.2078)\n",
            " loss:tensor(0.1001)\n",
            " loss:tensor(0.3097)\n",
            " loss:tensor(0.1160)\n",
            " loss:tensor(0.1473)\n",
            " loss:tensor(0.1340)\n",
            " loss:tensor(0.1059)\n",
            " loss:tensor(0.0819)\n",
            " loss:tensor(0.1136)\n",
            " loss:tensor(0.0838)\n",
            " loss:tensor(0.1044)\n",
            " loss:tensor(0.2386)\n",
            " loss:tensor(0.4879)\n",
            " loss:tensor(0.0790)\n",
            " loss:tensor(0.0853)\n",
            " loss:tensor(0.2046)\n",
            " loss:tensor(0.6608)\n",
            " loss:tensor(0.7399)\n",
            " loss:tensor(0.5823)\n",
            " loss:tensor(0.1433)\n",
            " loss:tensor(0.1479)\n",
            " loss:tensor(0.0943)\n",
            " loss:tensor(0.3023)\n",
            " loss:tensor(0.5575)\n",
            " loss:tensor(0.1069)\n",
            " loss:tensor(0.0925)\n",
            " loss:tensor(0.2522)\n",
            " loss:tensor(0.1023)\n",
            " loss:tensor(0.2813)\n",
            " loss:tensor(0.1027)\n",
            " loss:tensor(0.7063)\n",
            " loss:tensor(0.3405)\n",
            " loss:tensor(0.6704)\n",
            " loss:tensor(1.4568)\n",
            " loss:tensor(0.9693)\n",
            " loss:tensor(0.0864)\n",
            " loss:tensor(0.7179)\n",
            " loss:tensor(0.2877)\n",
            " loss:tensor(0.7329)\n",
            " loss:tensor(0.4606)\n",
            " loss:tensor(0.1361)\n",
            " loss:tensor(0.1750)\n",
            " loss:tensor(0.0859)\n",
            " loss:tensor(0.2495)\n",
            " loss:tensor(0.5155)\n",
            " loss:tensor(0.1662)\n",
            " loss:tensor(0.4831)\n",
            " loss:tensor(0.2722)\n",
            " loss:tensor(0.1664)\n",
            " loss:tensor(0.2765)\n",
            " loss:tensor(0.3199)\n",
            " loss:tensor(0.2408)\n",
            " loss:tensor(0.0676)\n",
            " loss:tensor(0.2341)\n",
            " loss:tensor(0.1589)\n",
            " loss:tensor(0.1958)\n",
            " loss:tensor(0.4688)\n",
            " loss:tensor(0.5671)\n",
            " loss:tensor(0.2713)\n",
            " loss:tensor(0.9161)\n",
            " loss:tensor(0.0833)\n",
            " loss:tensor(0.8860)\n",
            " loss:tensor(0.1107)\n",
            " loss:tensor(0.3750)\n",
            " loss:tensor(1.2760)\n",
            " loss:tensor(0.4635)\n",
            " loss:tensor(1.2198)\n",
            " loss:tensor(0.1106)\n",
            " loss:tensor(1.3627)\n",
            " loss:tensor(0.9184)\n",
            " loss:tensor(0.5038)\n",
            " loss:tensor(0.3884)\n",
            " loss:tensor(0.1241)\n",
            " loss:tensor(0.6118)\n",
            " loss:tensor(0.1595)\n",
            " loss:tensor(0.1441)\n",
            " loss:tensor(0.4810)\n",
            " loss:tensor(0.1150)\n",
            " loss:tensor(0.2560)\n",
            " loss:tensor(0.0861)\n",
            " loss:tensor(0.3333)\n",
            " loss:tensor(0.0605)\n",
            " loss:tensor(0.1253)\n",
            " loss:tensor(0.7772)\n",
            " loss:tensor(0.4667)\n",
            " loss:tensor(0.0857)\n",
            " loss:tensor(0.1563)\n",
            " loss:tensor(0.1965)\n",
            " loss:tensor(0.3034)\n",
            " loss:tensor(0.4336)\n",
            " loss:tensor(0.3567)\n",
            " loss:tensor(0.4755)\n",
            " loss:tensor(0.4703)\n",
            " loss:tensor(0.8938)\n",
            " loss:tensor(0.1094)\n",
            " loss:tensor(0.2132)\n",
            " loss:tensor(0.4197)\n",
            " loss:tensor(0.2926)\n",
            " loss:tensor(0.1242)\n",
            " loss:tensor(0.7191)\n",
            " loss:tensor(0.1389)\n",
            " loss:tensor(0.1306)\n",
            " loss:tensor(0.6748)\n",
            " loss:tensor(0.0489)\n",
            " loss:tensor(1.2633)\n",
            " loss:tensor(0.1528)\n",
            " loss:tensor(0.0854)\n",
            " loss:tensor(0.0665)\n",
            " loss:tensor(0.0966)\n",
            " loss:tensor(0.2683)\n",
            " loss:tensor(0.1572)\n",
            " loss:tensor(0.3135)\n",
            " loss:tensor(0.2828)\n",
            " loss:tensor(0.0928)\n",
            " loss:tensor(0.0752)\n",
            " loss:tensor(0.2923)\n",
            " loss:tensor(0.1118)\n",
            " loss:tensor(0.0982)\n",
            " loss:tensor(0.9780)\n",
            " loss:tensor(1.1214)\n",
            " loss:tensor(0.0984)\n",
            " loss:tensor(0.1134)\n",
            " loss:tensor(0.4848)\n",
            " loss:tensor(0.1013)\n",
            " loss:tensor(0.1979)\n",
            " loss:tensor(1.2780)\n",
            " loss:tensor(0.6607)\n",
            " loss:tensor(0.1866)\n",
            " loss:tensor(0.3463)\n",
            " loss:tensor(0.0481)\n",
            " loss:tensor(0.1881)\n",
            " loss:tensor(0.2537)\n",
            " loss:tensor(0.6941)\n",
            " loss:tensor(0.3242)\n",
            " loss:tensor(0.6050)\n",
            " loss:tensor(0.1155)\n",
            " loss:tensor(0.1813)\n",
            " loss:tensor(0.1043)\n",
            " loss:tensor(0.3219)\n",
            " loss:tensor(0.1521)\n",
            " loss:tensor(0.0831)\n",
            " loss:tensor(0.3127)\n",
            " loss:tensor(0.2261)\n",
            " loss:tensor(0.1536)\n",
            " loss:tensor(0.0868)\n",
            " loss:tensor(0.0905)\n",
            " loss:tensor(0.1284)\n",
            " loss:tensor(0.1622)\n",
            " loss:tensor(0.9327)\n",
            " loss:tensor(0.2343)\n",
            " loss:tensor(0.2068)\n",
            " loss:tensor(0.4422)\n",
            " loss:tensor(1.4155)\n",
            " loss:tensor(0.1315)\n",
            " loss:tensor(0.3729)\n",
            " loss:tensor(0.3291)\n",
            " loss:tensor(0.1128)\n",
            " loss:tensor(0.2696)\n",
            " loss:tensor(0.9319)\n",
            " loss:tensor(0.2361)\n",
            " loss:tensor(0.3552)\n",
            " loss:tensor(0.3121)\n",
            " loss:tensor(0.4835)\n",
            " loss:tensor(0.7865)\n",
            " loss:tensor(0.1509)\n",
            " loss:tensor(0.0896)\n",
            " loss:tensor(0.1686)\n",
            " loss:tensor(0.4045)\n",
            " loss:tensor(0.8404)\n",
            " loss:tensor(0.3758)\n",
            " loss:tensor(0.2083)\n",
            " loss:tensor(0.5130)\n",
            " loss:tensor(0.6815)\n",
            " loss:tensor(0.1178)\n",
            " loss:tensor(0.9634)\n",
            " loss:tensor(1.3174)\n",
            " loss:tensor(0.0554)\n",
            " loss:tensor(0.1719)\n",
            " loss:tensor(0.1593)\n",
            " loss:tensor(0.0606)\n",
            " loss:tensor(0.1151)\n",
            " loss:tensor(0.8965)\n",
            " loss:tensor(0.2339)\n",
            " loss:tensor(0.1141)\n",
            " loss:tensor(0.6707)\n",
            " loss:tensor(0.1472)\n",
            " loss:tensor(0.1818)\n",
            " loss:tensor(0.6060)\n",
            " loss:tensor(0.6708)\n",
            " loss:tensor(0.4431)\n",
            " loss:tensor(0.5532)\n",
            " loss:tensor(0.2826)\n",
            " loss:tensor(0.2752)\n",
            " loss:tensor(0.1460)\n",
            " loss:tensor(0.0964)\n",
            " loss:tensor(0.6397)\n",
            " loss:tensor(1.2970)\n",
            " loss:tensor(0.2723)\n",
            " loss:tensor(0.6590)\n",
            " loss:tensor(0.1673)\n",
            " loss:tensor(0.2780)\n",
            " loss:tensor(0.2009)\n",
            " loss:tensor(0.1046)\n",
            " loss:tensor(0.0536)\n",
            " loss:tensor(0.6111)\n",
            " loss:tensor(0.1106)\n",
            " loss:tensor(0.3849)\n",
            " loss:tensor(0.7935)\n",
            " loss:tensor(1.3457)\n",
            " loss:tensor(0.4434)\n",
            " loss:tensor(0.4381)\n",
            " loss:tensor(0.2759)\n",
            " loss:tensor(0.1022)\n",
            " loss:tensor(0.8553)\n",
            " loss:tensor(0.1083)\n",
            " loss:tensor(0.0821)\n",
            " loss:tensor(0.1396)\n",
            " loss:tensor(0.3395)\n",
            " loss:tensor(1.0855)\n",
            " loss:tensor(0.0816)\n",
            " loss:tensor(0.7464)\n",
            " loss:tensor(0.1196)\n",
            " loss:tensor(0.3255)\n",
            " loss:tensor(0.8683)\n",
            " loss:tensor(0.4885)\n",
            " loss:tensor(0.4943)\n",
            " loss:tensor(0.1089)\n",
            " loss:tensor(0.4531)\n",
            " loss:tensor(0.5897)\n",
            " loss:tensor(0.1926)\n",
            " loss:tensor(1.0924)\n",
            " loss:tensor(0.2252)\n",
            " loss:tensor(0.1264)\n",
            " loss:tensor(0.1850)\n",
            " loss:tensor(0.1260)\n",
            " loss:tensor(0.1768)\n",
            " loss:tensor(0.5129)\n",
            " loss:tensor(0.6127)\n",
            " loss:tensor(0.7269)\n",
            " loss:tensor(0.2081)\n",
            " loss:tensor(0.0726)\n",
            " loss:tensor(0.8756)\n",
            " loss:tensor(1.1389)\n",
            " loss:tensor(1.0960)\n",
            " loss:tensor(0.8820)\n",
            " loss:tensor(1.0110)\n",
            " loss:tensor(0.3591)\n",
            " loss:tensor(0.1062)\n",
            " loss:tensor(0.2808)\n",
            " loss:tensor(0.3861)\n",
            " loss:tensor(1.9412)\n",
            " loss:tensor(0.1907)\n",
            " loss:tensor(0.1518)\n",
            " loss:tensor(1.6312)\n",
            " loss:tensor(1.5041)\n",
            " loss:tensor(0.0879)\n",
            " loss:tensor(0.0758)\n",
            " loss:tensor(0.3064)\n",
            " loss:tensor(0.1224)\n",
            " loss:tensor(0.1539)\n",
            " loss:tensor(1.2077)\n",
            " loss:tensor(0.3763)\n",
            " loss:tensor(0.3543)\n",
            " loss:tensor(0.1127)\n",
            " loss:tensor(0.5017)\n",
            " loss:tensor(1.2383)\n",
            " loss:tensor(0.6555)\n",
            " loss:tensor(1.1893)\n",
            " loss:tensor(0.4566)\n",
            " loss:tensor(0.9976)\n",
            " loss:tensor(0.7954)\n",
            " loss:tensor(0.6691)\n",
            " loss:tensor(0.7020)\n",
            " loss:tensor(0.9054)\n",
            " loss:tensor(0.0904)\n",
            " loss:tensor(1.3083)\n",
            " loss:tensor(0.0458)\n",
            " loss:tensor(1.6147)\n",
            " loss:tensor(0.1054)\n",
            " loss:tensor(0.6242)\n",
            " loss:tensor(0.1851)\n",
            " loss:tensor(0.5737)\n",
            " loss:tensor(1.4117)\n",
            " loss:tensor(0.0825)\n",
            " loss:tensor(0.1723)\n",
            " loss:tensor(1.1631)\n",
            " loss:tensor(1.0939)\n",
            " loss:tensor(1.0075)\n",
            " loss:tensor(1.0639)\n",
            " loss:tensor(0.3717)\n",
            " loss:tensor(0.2810)\n",
            " loss:tensor(0.0827)\n",
            " loss:tensor(1.0058)\n",
            " loss:tensor(0.0899)\n",
            " loss:tensor(0.6710)\n",
            " loss:tensor(0.4259)\n",
            " loss:tensor(0.2514)\n",
            " loss:tensor(0.3581)\n",
            " loss:tensor(0.2642)\n",
            " loss:tensor(0.4845)\n",
            " loss:tensor(0.8758)\n",
            " loss:tensor(1.1595)\n",
            " loss:tensor(1.5389)\n",
            " loss:tensor(0.2099)\n",
            " loss:tensor(0.7068)\n",
            " loss:tensor(0.2755)\n",
            " loss:tensor(0.9952)\n",
            " loss:tensor(0.1603)\n",
            " loss:tensor(0.9851)\n",
            " loss:tensor(0.0833)\n",
            " loss:tensor(0.4656)\n",
            " loss:tensor(0.1389)\n",
            " loss:tensor(0.3803)\n",
            " loss:tensor(1.3189)\n",
            " loss:tensor(0.2108)\n",
            " loss:tensor(0.0918)\n",
            " loss:tensor(0.9188)\n",
            " loss:tensor(1.3430)\n",
            " loss:tensor(0.6619)\n",
            " loss:tensor(0.8732)\n",
            " loss:tensor(0.7521)\n",
            " loss:tensor(0.8705)\n",
            " loss:tensor(0.2471)\n",
            " loss:tensor(0.1131)\n",
            " loss:tensor(0.3404)\n",
            " loss:tensor(0.1928)\n",
            " loss:tensor(0.3164)\n",
            " loss:tensor(0.1064)\n",
            " loss:tensor(0.8776)\n",
            " loss:tensor(0.0760)\n",
            " loss:tensor(0.0871)\n",
            " loss:tensor(0.3025)\n",
            " loss:tensor(0.1236)\n",
            " loss:tensor(0.1130)\n",
            " loss:tensor(0.4146)\n",
            " loss:tensor(0.4608)\n",
            " loss:tensor(0.0767)\n",
            " loss:tensor(1.5423)\n",
            " loss:tensor(0.0593)\n",
            " loss:tensor(0.2366)\n",
            " loss:tensor(0.2076)\n",
            " loss:tensor(0.2423)\n",
            " loss:tensor(0.1075)\n",
            " loss:tensor(0.0802)\n",
            " loss:tensor(0.3260)\n",
            " loss:tensor(0.3905)\n",
            " loss:tensor(0.2716)\n",
            " loss:tensor(1.0119)\n",
            " loss:tensor(1.5005)\n",
            " loss:tensor(0.1254)\n",
            " loss:tensor(0.1160)\n",
            " loss:tensor(0.1142)\n",
            " loss:tensor(1.5347)\n",
            " loss:tensor(0.9658)\n",
            " loss:tensor(0.4179)\n",
            " loss:tensor(0.4792)\n",
            " loss:tensor(0.1837)\n",
            " loss:tensor(0.3733)\n",
            " loss:tensor(0.1304)\n",
            " loss:tensor(1.0766)\n",
            " loss:tensor(0.1032)\n",
            " loss:tensor(1.2188)\n",
            " loss:tensor(0.2648)\n",
            " loss:tensor(1.1190)\n",
            " loss:tensor(0.1567)\n",
            " loss:tensor(0.4264)\n",
            " loss:tensor(0.4054)\n",
            " loss:tensor(1.6622)\n",
            " loss:tensor(0.6828)\n",
            " loss:tensor(0.2013)\n",
            " loss:tensor(0.2063)\n",
            " loss:tensor(0.4438)\n",
            " loss:tensor(0.8908)\n",
            " loss:tensor(1.5410)\n",
            " loss:tensor(0.0625)\n",
            " loss:tensor(0.4335)\n",
            " loss:tensor(0.1970)\n",
            " loss:tensor(0.1993)\n",
            " loss:tensor(0.1068)\n",
            " loss:tensor(1.5530)\n",
            " loss:tensor(1.3605)\n",
            " loss:tensor(1.0040)\n",
            " loss:tensor(0.1161)\n",
            " loss:tensor(0.4615)\n",
            " loss:tensor(0.0972)\n",
            " loss:tensor(0.2108)\n",
            " loss:tensor(0.1677)\n",
            " loss:tensor(0.2335)\n",
            " loss:tensor(0.1180)\n",
            " loss:tensor(0.0986)\n",
            " loss:tensor(2.6059)\n",
            " loss:tensor(1.5019)\n",
            " loss:tensor(0.0682)\n",
            " loss:tensor(0.0935)\n",
            " loss:tensor(0.1271)\n",
            " loss:tensor(0.3094)\n",
            " loss:tensor(0.2172)\n",
            " loss:tensor(0.1549)\n",
            " loss:tensor(0.0676)\n",
            " loss:tensor(0.4761)\n",
            " loss:tensor(0.4365)\n",
            " loss:tensor(0.1098)\n",
            " loss:tensor(0.1298)\n",
            " loss:tensor(1.3887)\n",
            " loss:tensor(0.0726)\n",
            " loss:tensor(0.0711)\n",
            " loss:tensor(0.2163)\n",
            " loss:tensor(0.0778)\n",
            " loss:tensor(0.1230)\n",
            " loss:tensor(1.5722)\n",
            " loss:tensor(0.7266)\n",
            " loss:tensor(1.3765)\n",
            " loss:tensor(0.2081)\n",
            " loss:tensor(0.1194)\n",
            " loss:tensor(0.1338)\n",
            " loss:tensor(0.4100)\n",
            " loss:tensor(0.0853)\n",
            " loss:tensor(0.1426)\n",
            " loss:tensor(0.0875)\n",
            " loss:tensor(0.4849)\n",
            " loss:tensor(0.1082)\n",
            " loss:tensor(0.1956)\n",
            " loss:tensor(1.4371)\n",
            " loss:tensor(0.1060)\n",
            " loss:tensor(0.6434)\n",
            " loss:tensor(0.3922)\n",
            " loss:tensor(0.7532)\n",
            " loss:tensor(0.4791)\n",
            " loss:tensor(0.6586)\n",
            " loss:tensor(1.6061)\n",
            " loss:tensor(0.7510)\n",
            " loss:tensor(0.2101)\n",
            " loss:tensor(0.1672)\n",
            " loss:tensor(0.1028)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}