{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Text Generation .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2qLqDd+kFzu9k+kKowDUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumathiGit/Python-World/blob/main/RNN_Text_Generation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RM0HXHRWi1K"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.cuda as cuda\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXHW0AWtbWBx"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        \n",
        "        # This is very english language specific\n",
        "        # We will ingest only these characters:\n",
        "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
        "        \n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                line = ''.join([c for c in line if c in self.whitelist])\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsy5EsVQbfpJ",
        "outputId": "83d1df7b-8681-4513-ea29-5d7eea5192e6"
      },
      "source": [
        "!ls data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.txt  valid.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwV67-VubnXh"
      },
      "source": [
        "corpus = Corpus('./data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxrTXOL9bvF7",
        "outputId": "bd6f1d00-3938-4bf9-9a99-9d3663185e16"
      },
      "source": [
        "print(corpus.dictionary.idx2word[10])                                          #We can able to view a particular vocabulary by their index value \n",
        "print(corpus.dictionary.word2idx['That'])                                      #Here we can get the index value of a particular word itself"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKQfsLMAb2rh",
        "outputId": "67cf111c-d4fb-4b04-936a-5973244080cc"
      },
      "source": [
        "print(corpus.train.size())\n",
        "print(corpus.valid.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1039900])\n",
            "torch.Size([63420])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lZM8tafyb7P_",
        "outputId": "f2ba9ec6-a7fb-4eed-bde2-cb3e3d2dd71f"
      },
      "source": [
        "id = corpus.train[112]                                                                                        #Here the trained dataset with a index of 112 contains the word else\n",
        "corpus.dictionary.idx2word[id]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'else'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFtKgh1-b7dV",
        "outputId": "4172e13a-6b60-4803-ae44-d80baff01629"
      },
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "print(vocab_size)                                                        #There are 74010 vocabulary in this corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_I3DVpecDVr"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):                                       #We are applying dropout for the model do not get overfitting\n",
        "        \n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Embedding(vocab_size, embed_size)                                                                #First hidden layer in RNN is an Embeeded layer which has three arguments (input dimensions)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop1(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop2(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQLY3-AEcDh5"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    if cuda.is_available():\n",
        "        data = data.cuda()\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhB3HY3-cM-b",
        "outputId": "b4a31e66-ad87-47f3-cc7f-0a69ec3af9b2"
      },
      "source": [
        "dummy_data = \"Once upon a time there was a good king and a queen\"\n",
        "dummy_data_idx = [corpus.dictionary.word2idx[w] for w in dummy_data.split()]\n",
        "dummy_tensor = torch.LongTensor(dummy_data_idx) \n",
        "op = batchify(dummy_tensor, 2)                             #2 indicates the batch size >> the output contains two batches as you can see\n",
        "for row in op:\n",
        "    print(\"%10s %10s\" %  (corpus.dictionary.idx2word[row[0]], corpus.dictionary.idx2word[row[1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Once          a\n",
            "      upon       good\n",
            "         a       king\n",
            "      time        and\n",
            "     there          a\n",
            "       was      queen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB1vDjoWcYaz"
      },
      "source": [
        "bs_train = 20       # batch size for training set\n",
        "bs_valid = 10       # batch size for validation set\n",
        "bptt_size = 35      # number of times to unroll the graph for back propagation through time\n",
        "clip = 0.25         # gradient clipping to check exploding gradient\n",
        "\n",
        "embed_size = 200    # size of the embedding vector\n",
        "hidden_size = 200   # size of the hidden state in the RNN \n",
        "num_layers = 2      # number of RNN layres to use\n",
        "dropout_pct = 0.5   # %age of neurons to drop out for regularization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIe1OyadcYp0"
      },
      "source": [
        "train_data = batchify(corpus.train, bs_train)\n",
        "val_data = batchify(corpus.valid, bs_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdFUcld6cY10",
        "outputId": "0bc7c334-e656-4872-fb23-7bde2458850a"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([51995, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zyElxW7cZAK"
      },
      "source": [
        "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
        "\n",
        "if cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnz4IyFFcmyq"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ2LH_5ncp4C"
      },
      "source": [
        "def get_batch(source, i, evaluation=False):\n",
        "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
        "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
        "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
        "    if cuda.is_available():\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5-uIZkEctqZ"
      },
      "source": [
        "data, target = get_batch(train_data, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI9SSrpWct2e",
        "outputId": "1be5df09-5d29-457f-fcb6-94b7a2c6f3f5"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([35, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Lobq5pcuBx",
        "outputId": "9d057edc-defb-4937-a376-d165a12a4501"
      },
      "source": [
        "target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([700])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLRTAklJc43m"
      },
      "source": [
        "def train(data_source, lr):\n",
        "    # Turn on training mode which enables dropout.\n",
        "    \n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    hidden = model.init_hidden(bs_train)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt_size)):                                                 #Here we are using enumurate to get batch ids instead of fetching batches\n",
        "        \n",
        "        data, targets = get_batch(data_source, i)                                                                            #Passing the index as well as the source to get the target as well as the output data\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = Variable(hidden.data)\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "        \n",
        "        # model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.data\n",
        "        \n",
        "    return total_loss / len(data_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhYek9hc9EX"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout. \n",
        "    model.eval()                                 #Put the model in the evaluation mode for validation set here there are no optimizers \n",
        "    total_loss = 0\n",
        "    hidden = model.init_hidden(bs_valid)\n",
        "    \n",
        "    for i in range(0, data_source.size(0) - 1, bptt_size):\n",
        "        data, targets = get_batch(data_source, i, evaluation=True)\n",
        "        \n",
        "        if cuda.is_available():\n",
        "            hidden = hidden.cuda()\n",
        "            \n",
        "        output, hidden = model(data, hidden)\n",
        "        output_flat = output.view(-1, vocab_size)\n",
        "        \n",
        "        total_loss += len(data) * criterion(output_flat, targets).data\n",
        "        hidden = Variable(hidden.data)\n",
        "        \n",
        "    return total_loss / len(data_source)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9QHkZlNdBIT"
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4agWv-KdBR7"
      },
      "source": [
        "def run(epochs, lr):\n",
        "    global best_val_loss\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        train_loss = train(train_data, lr)\n",
        "        val_loss = evaluate(val_data)\n",
        "        print(\"Train Loss: \", train_loss, \"Valid Loss: \", val_loss)\n",
        "\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"./4.model.pth\")                                                #we are saving the best val loss in the file system so we can access the model using the file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3w4VHjCdJDH",
        "outputId": "179109da-43ec-47ff-d5e4-f5eb3c2897d7"
      },
      "source": [
        "run(5, 0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss:  tensor(6.1155, device='cuda:0') Valid Loss:  tensor(6.7295, device='cuda:0')\n",
            "Train Loss:  tensor(6.0440, device='cuda:0') Valid Loss:  tensor(6.7363, device='cuda:0')\n",
            "Train Loss:  tensor(5.9873, device='cuda:0') Valid Loss:  tensor(6.7455, device='cuda:0')\n",
            "Train Loss:  tensor(5.9433, device='cuda:0') Valid Loss:  tensor(6.7667, device='cuda:0')\n",
            "Train Loss:  tensor(5.9047, device='cuda:0') Valid Loss:  tensor(6.7848, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFConxUGdJe3",
        "outputId": "4567456d-014c-4aef-c442-2f82704586ae"
      },
      "source": [
        "run(5, 0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Loss:  tensor(5.8783, device='cuda:0') Valid Loss:  tensor(6.7926, device='cuda:0')\n",
            "Train Loss:  tensor(5.8554, device='cuda:0') Valid Loss:  tensor(6.8223, device='cuda:0')\n",
            "Train Loss:  tensor(5.8385, device='cuda:0') Valid Loss:  tensor(6.8556, device='cuda:0')\n",
            "Train Loss:  tensor(5.8248, device='cuda:0') Valid Loss:  tensor(6.8836, device='cuda:0')\n",
            "Train Loss:  tensor(5.8185, device='cuda:0') Valid Loss:  tensor(6.8931, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMvhug1RdQ9D"
      },
      "source": [
        "num_words = 200\n",
        "temperature = 1                                                            #Gramatically correct sentences >> temparature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ff4bApidRSb",
        "outputId": "a5e04ece-2a38-4d20-fa38-72a9a8b37c8b"
      },
      "source": [
        "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
        "model.load_state_dict(torch.load(\"./4.model.pth\"))                                                           #Passing the trained file system for evalation\n",
        "\n",
        "if cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (encoder): Embedding(74010, 200)\n",
              "  (drop1): Dropout(p=0.5, inplace=False)\n",
              "  (drop2): Dropout(p=0.5, inplace=False)\n",
              "  (rnn): GRU(200, 200, num_layers=2, dropout=0.5)\n",
              "  (decoder): Linear(in_features=200, out_features=74010, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6wRS7sxdXla"
      },
      "source": [
        "# https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
        "# Which sample is better? It depends on your personal taste. The high temperature \n",
        "# sample displays greater linguistic variety, but the low temperature sample is \n",
        "# more grammatically correct. Such is the world of temperature sampling - lowering \n",
        "# the temperature allows you to focus on higher probability output sequences and \n",
        "# smooth over deficiencies of the model.\n",
        "\n",
        "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
        "# Often we want to sample with low temperatures to produce sharp probabilities\n",
        "temperature = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e1lt6f7dXwz",
        "outputId": "518fb193-3fb1-4486-8d13-27ab45cddfc1"
      },
      "source": [
        "hidden = model.init_hidden(1)                                                #Initially it was zero in hidden we are initializing with 1\n",
        "idx = corpus.dictionary.word2idx['Beautiful']                                        #The word we want to start with \n",
        "input = Variable(torch.LongTensor([[idx]]).long(), volatile=True)\n",
        "\n",
        "if cuda.is_available():\n",
        "    input.data = input.data.cuda()\n",
        "\n",
        "print(corpus.dictionary.idx2word[idx], '', end='')\n",
        "\n",
        "for i in range(num_words):                                                     #The num of words we are setting is about 200 i this case \n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.data.fill_(word_idx)\n",
        "    word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "    if word == '<eos>':\n",
        "        print('')\n",
        "    else:\n",
        "        print(word + ' ', end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beautiful \n",
            "QUEEN. Your FRANCE. \n",
            "Indeed, the better is to him. \n",
            "PISTOL. I shall not be it a matter? \n",
            "EMILIA. I would not to you be to your bags; \n",
            "And for the true my kind of parting before \n",
            "The very mind; I have lost mine day, \n",
            "And now I could be in a dark \n",
            "That tonight. \n",
            "TRINCULO. With this ere the poor officer, \n",
            "Hath thou no gods not desire to kill the sweet. \n",
            "If you see your difference to see me. \n",
            "BEROWNE. Why was a gentleman of such a man; \n",
            "Which that have power upon my Highness Yet \n",
            "The noble sun that saw their stir to victory. \n",
            "My heart is with him. \n",
            "CELIA. It is the simple Let's to all well; and I have my name? \n",
            "ISABELLA. I'll bear our little common eye of roughly to \n",
            "strength to choke it. I have call'd the herd of thee. \n",
            "ESCALUS. Here is the place of your Grace but whom he will have to \n",
            "his love. \n",
            "POMPEY. Then I will do sure stay upon your 5. \n",
            "TITUS. If he I should save this "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}